{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165f227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b308a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a87b619e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '../Code'\n",
    "sys.path.append('../Code/NTU-FN6903-Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d443661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b83086",
   "metadata": {},
   "source": [
    "# Merge Datasets into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91fb9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of data files\n",
    "DATA_DIR = '../Data'\n",
    "\n",
    "# files in DATA_DIR\n",
    "filetype_lst = ['DATA', 'VAX', 'SYMPTOMS']\n",
    "\n",
    "# create dictionary of 3 dataframes\n",
    "# each dataframe combines the \"DATA\"/\"VAX\"/\"SYMPTOMS\" of years 2020-2022\n",
    "df_dict = {\n",
    "    ft: pd.concat(\n",
    "        [pd.read_csv(\n",
    "            f,\n",
    "            encoding=\"ISO-8859-1\", \n",
    "            low_memory=False\n",
    "        ) for f in glob.glob(os.path.join(DATA_DIR, f'*{ft}.csv'))\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    ) for ft in filetype_lst\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d080a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of vaccinations from the VAX dataframe\n",
    "# (df_dict['VAX'].VAX_TYPE.value_counts()/df_dict['VAX'].shape[0]).head(10)\n",
    "\n",
    "#COVID19      0.848469\n",
    "#VARZOS       0.032324\n",
    "#FLU4         0.016010\n",
    "#UNK          0.015465\n",
    "#COVID19-2    0.013953\n",
    "#PPV          0.004970\n",
    "#FLUX         0.004820\n",
    "#VARCEL       0.004585\n",
    "#HPV9         0.004405\n",
    "#TDAP         0.004321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa7acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join symptoms into continuous string\n",
    "df_dict['SYMPTOMS']['SYMP_STR'] = (\n",
    "    df_dict['SYMPTOMS']\n",
    "    .drop(columns=df_dict['SYMPTOMS'].filter(regex='VERSION'))\n",
    "    .filter(regex='SYMPTOM')\n",
    "    .apply(lambda s: '|'.join(filter(pd.notna, s)), axis=1)\n",
    ")\n",
    "\n",
    "# combine symptoms for each unique VAERS ID\n",
    "df_dict['SYMPTOMS'] = (\n",
    "    df_dict['SYMPTOMS']\n",
    "    .groupby('VAERS_ID')\n",
    "    .agg({'SYMP_STR': '|'.join})[\n",
    "        'SYMP_STR'\n",
    "    ]\n",
    "    .str.split('|')\n",
    "    .apply(lambda s: ', '.join(set(s)))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2494a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the COVID19 vaccinations \n",
    "# merge VAX data with other dataframes on VAERS_ID (identifies unique patient)\n",
    "df_merged = (\n",
    "    df_dict['VAX'].loc[df_dict['VAX']['VAX_TYPE'] == 'COVID19']\n",
    "    .merge(df_dict['DATA'], how='left', on='VAERS_ID')\n",
    "    .merge(df_dict['SYMPTOMS'], how='left', on='VAERS_ID')\n",
    "    .sort_values('VAERS_ID')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924248ef",
   "metadata": {},
   "source": [
    "# Preprocess Numeric and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aeb260f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#COLS_DROP = \"\"\"VAX_TYPE, VAX_LOT, VAX_DOSE_SERIES, VAX_ROUTE, VAX_SITE, VAX_NAME, STATE, CAGE_YR, CAGE_MO, CUR_ILL, LAB_DATA, V_ADMINBY, V_FUNDBY, SPLTTYPE, FORM_VERS, OFC_VISIT, ER_ED_VISIT\"\"\".split(', ')\n",
    "#COLS_BINARY = \"\"\"DIED, L_THREAT, ER_VISIT, HOSPITAL, X_STAY, DISABLE, BIRTH_DEFECT\"\"\".split(', ')\n",
    "COLS_DATE = \"\"\"RECVDATE, RPT_DATE, DATEDIED, VAX_DATE, ONSET_DATE, TODAYS_DATE\"\"\".split(', ')\n",
    "#COLS_TEXT = \"\"\"HISTORY, ALLERGIES\"\"\".split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6a5e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat dates\n",
    "df_merged[COLS_DATE] = df_merged[COLS_DATE].apply(pd.to_datetime, format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# drop duplicated entries\n",
    "df_merged = df_merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# preprocess numeric variables\n",
    "\n",
    "# flag errors in vax_date and onset_date\n",
    "df_merged.loc[\n",
    "    (df_merged['ONSET_DATE'] < pd.to_datetime('2020-01-01')) |\n",
    "    (df_merged['VAX_DATE'] < pd.to_datetime('2020-01-01')) | \n",
    "    ((df_merged['ONSET_DATE'] - df_merged['VAX_DATE']).astype('timedelta64[D]') != df_merged['NUMDAYS']),\n",
    "    ['VAX_DATE', 'ONSET_DATE']\n",
    "] = pd.NaT\n",
    "\n",
    "# flag errors in numdays\n",
    "df_merged.loc[\n",
    "    df_merged['ONSET_DATE'].isna() |\n",
    "    df_merged['VAX_DATE'].isna(),\n",
    "    'NUMDAYS'\n",
    "] = np.nan\n",
    "\n",
    "# drop ALL rows with na values in either numdays or age\n",
    "df_merged.dropna(subset=['NUMDAYS', 'AGE_YRS', 'SYMP_STR'], inplace=True)\n",
    "\n",
    "\n",
    "# preprocess categorical variables\n",
    "\n",
    "# VAX_MANU\n",
    "df_merged['MANU_PFIZER'] = np.where(df_merged['VAX_MANU'] == 'PFIZER\\BIONTECH', 1, 0)\n",
    "df_merged['MANU_MODERNA'] = np.where(df_merged['VAX_MANU'] == 'MODERNA', 1, 0)\n",
    "df_merged['MANU_JANSSEN'] = np.where(df_merged['VAX_MANU'] == 'JANSSEN', 1, 0)\n",
    "df_merged['MANU_NOVAVAX'] = np.where(df_merged['VAX_MANU'] == 'NOVAVAX', 1, 0)\n",
    "\n",
    "# GENDER\n",
    "df_merged['GENDER_F'] = np.where(df_merged['SEX']=='F', 1, 0)\n",
    "df_merged['GENDER_M'] = np.where(df_merged['SEX']=='M', 1, 0)\n",
    "\n",
    "# BIRTH_DEFECT\n",
    "df_merged['DEFECT'] = np.where(df_merged['BIRTH_DEFECT'] == 'Y', 1, 0)\n",
    "\n",
    "# HOSPITAL (response variable)\n",
    "df_merged['HOSP'] = np.where(df_merged['HOSPITAL'] == 'Y', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174ee8c",
   "metadata": {},
   "source": [
    "# Preprocess 'HISTORY' strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec4e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace na values in medical history\n",
    "df_merged['HISTORY'] = df_merged['HISTORY'].str.lower().str.strip()\n",
    "df_merged.loc[df_merged['HISTORY'].isna(), 'HISTORY'] = 'no known medical history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf65133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sBERT model\n",
    "# model = SentenceTransformer('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45ef3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify cases of no known medical history cases\n",
    "#nkmh_phrases_count = df_merged.loc[df_merged['HISTORY'].astype(str).apply(len) <= 1000, 'HISTORY'].value_counts()\n",
    "#nkmh_phrases = list(nkmh_phrases_count[nkmh_phrases_count>=5].index)\n",
    "\n",
    "#nkmh_phrases_embeddings = model.encode(nkmh_phrases, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b604ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain embeddings for reference phrases\n",
    "#nkmh_references_embeddings = model.encode([\n",
    "#    'no known medical history',\n",
    "#    'none reported',\n",
    "#    'unknown',\n",
    "#    'no adverse',\n",
    "#    'unaware',\n",
    "#    'none'\n",
    "#], show_progress_bar=True)\n",
    "\n",
    "## normalise embeddings of reference phrases\n",
    "#nkmh_references_embeddings = nkmh_references_embeddings / np.linalg.norm(nkmh_references_embeddings, axis=1)[:, None]\n",
    "\n",
    "## cosine similarities of all embeddings to reference phrases\n",
    "#nkmh_cos_sim = np.matmul(nkmh_references_embeddings, (nkmh_phrases_embeddings / np.linalg.norm(nkmh_phrases_embeddings, axis=1)[:, None]).T)\n",
    "\n",
    "## rank by embeddings by cosine similarity to reference phrases to identify all synomymous phrases\n",
    "#(\n",
    "#    pd.DataFrame({\n",
    "#        'Phrase': nkmh_phrases,\n",
    "#        'Cosine Similarity with NKMH': np.max(nkmh_cos_sim, axis=0)\n",
    "#    })\n",
    "#    .sort_values('Cosine Similarity with NKMH', ascending=False)\n",
    "#    .to_excel(os.path.join(DATA_DIR, 'nkmh_phrases.xlsx'))\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff537e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synonymous phrases to nkmh using annotated excel file\n",
    "# file is generated by the above code and annotated manually\n",
    "nkmh_df = pd.read_excel(os.path.join(DATA_DIR, 'nkmh_phrases_annotated.xlsx'), keep_default_na=False)\n",
    "HISTORY_NKMH_PHRASES = nkmh_df.loc[nkmh_df['NKMH']=='Y', 'Phrase'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20501b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then rename nkmh phrases\n",
    "df_merged.loc[df_merged['HISTORY'].isin(HISTORY_NKMH_PHRASES), 'HISTORY'] = 'no known medical history'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee973fb",
   "metadata": {},
   "source": [
    "# Train-Test Split, then Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "866c199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged.sort_values(['ONSET_DATE', 'VAERS_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "132a71ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(802876, 51)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38cbc65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_X = 'ONSET_DATE, NUMDAYS, AGE_YRS, GENDER_F, GENDER_M, MANU_PFIZER, MANU_MODERNA, MANU_JANSSEN, MANU_NOVAVAX, DEFECT, HISTORY, SYMP_STR'.split(', ')\n",
    "COLS_Y = 'HOSP'.split(', ')\n",
    "\n",
    "X, y = df_merged[COLS_X], df_merged[COLS_Y]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=55, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "267b9014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save files\n",
    "#X_train.to_csv(os.path.join(DATA_DIR, 'X_train.csv'))\n",
    "#y_train.to_csv(os.path.join(DATA_DIR, 'y_train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce18fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test.to_csv(os.path.join(DATA_DIR, 'X_test.csv'))\n",
    "#y_test.to_csv(os.path.join(DATA_DIR, 'y_test.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
