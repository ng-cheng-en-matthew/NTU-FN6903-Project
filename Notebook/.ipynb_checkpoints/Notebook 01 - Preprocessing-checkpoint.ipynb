{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "165f227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "647eaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap, hdbscan\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from hyperopt import hp\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d1b0d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PATH'] = '../Code'\n",
    "sys.path.append('../Code/NTU-FN6903-Project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c3fbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustering import generate_clusters\n",
    "from tf_idf import generate_topic_df, c_tf_idf, extract_top_n_words_by_topic\n",
    "from param_search import bayesian_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d443661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b83086",
   "metadata": {},
   "source": [
    "# Merge Datasets into One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91fb9e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of data files\n",
    "DATA_DIR = '../Data'\n",
    "\n",
    "# files in DATA_DIR\n",
    "\n",
    "#['2022VAERSData.zip',\n",
    "# 'nkmh_phrases.xlsx',\n",
    "# 'nkmh_phrases_annotated.xlsx',\n",
    "# '.DS_Store',\n",
    "# '2022VAERSDATA.csv',\n",
    "# '2022VAERSSYMPTOMS.csv',\n",
    "# '2020VAERSVAX.csv',\n",
    "# '2022VAERSVAX.csv',\n",
    "# '2020VAERSSYMPTOMS.csv',\n",
    "# '2020VAERSDATA.csv',\n",
    "# '2021VAERSData.zip',\n",
    "# '2021VAERSVAX.csv',\n",
    "# '2020VAERSData.zip',\n",
    "# '2021VAERSDATA.csv',\n",
    "# '2021VAERSSYMPTOMS.csv']\n",
    "\n",
    "filetype_lst = ['DATA', 'VAX', 'SYMPTOMS']\n",
    "\n",
    "# create dictionary of 3 dataframes\n",
    "# each dataframe combines the \"DATA\"/\"VAX\"/\"SYMPTOMS\" of years 2020-2022\n",
    "df_dict = {\n",
    "    ft: pd.concat(\n",
    "        [pd.read_csv(\n",
    "            f,\n",
    "            encoding=\"ISO-8859-1\", \n",
    "            low_memory=False\n",
    "        ) for f in glob.glob(os.path.join(DATA_DIR, f'*{ft}.csv'))\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    ) for ft in filetype_lst\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d080a96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# types of vaccinations from the VAX dataframe\n",
    "# (df_dict['VAX'].VAX_TYPE.value_counts()/df_dict['VAX'].shape[0]).head(10)\n",
    "\n",
    "#COVID19      0.848469\n",
    "#VARZOS       0.032324\n",
    "#FLU4         0.016010\n",
    "#UNK          0.015465\n",
    "#COVID19-2    0.013953\n",
    "#PPV          0.004970\n",
    "#FLUX         0.004820\n",
    "#VARCEL       0.004585\n",
    "#HPV9         0.004405\n",
    "#TDAP         0.004321"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efa7acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join symptoms into continuous string\n",
    "df_dict['SYMPTOMS']['SYMP_STR'] = (\n",
    "    df_dict['SYMPTOMS']\n",
    "    .drop(columns=df_dict['SYMPTOMS'].filter(regex='VERSION'))\n",
    "    .filter(regex='SYMPTOM')\n",
    "    .apply(lambda s: '|'.join(filter(pd.notna, s)), axis=1)\n",
    ")\n",
    "\n",
    "# combine symptoms for each unique VAERS ID\n",
    "df_dict['SYMPTOMS'] = (\n",
    "    df_dict['SYMPTOMS']\n",
    "    .groupby('VAERS_ID')\n",
    "    .agg({'SYMP_STR': '|'.join})[\n",
    "        'SYMP_STR'\n",
    "    ]\n",
    "    .str.split('|')\n",
    "    .apply(lambda s: ', '.join(set(s)))\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2494a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the COVID19 vaccinations \n",
    "# merge VAX data with other dataframes on VAERS_ID (identifies unique patient)\n",
    "df_merged = (\n",
    "    df_dict['VAX'].loc[df_dict['VAX']['VAX_TYPE'] == 'COVID19']\n",
    "    .merge(df_dict['DATA'], how='left', on='VAERS_ID')\n",
    "    .merge(df_dict['SYMPTOMS'], how='left', on='VAERS_ID')\n",
    "    .sort_values('VAERS_ID')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76157a",
   "metadata": {},
   "source": [
    "# Preprocess Numeric and Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeb260f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#COLS_DROP = \"\"\"VAX_TYPE, VAX_LOT, VAX_DOSE_SERIES, VAX_ROUTE, VAX_SITE, VAX_NAME, STATE, CAGE_YR, CAGE_MO, CUR_ILL, LAB_DATA, V_ADMINBY, V_FUNDBY, SPLTTYPE, FORM_VERS, OFC_VISIT, ER_ED_VISIT\"\"\".split(', ')\n",
    "#COLS_BINARY = \"\"\"DIED, L_THREAT, ER_VISIT, HOSPITAL, X_STAY, DISABLE, BIRTH_DEFECT\"\"\".split(', ')\n",
    "COLS_DATE = \"\"\"RECVDATE, RPT_DATE, DATEDIED, VAX_DATE, ONSET_DATE, TODAYS_DATE\"\"\".split(', ')\n",
    "#COLS_TEXT = \"\"\"HISTORY, ALLERGIES\"\"\".split(', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1021bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat dates\n",
    "df_merged[COLS_DATE] = df_merged[COLS_DATE].apply(pd.to_datetime, format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "# drop duplicated entries\n",
    "df_merged = df_merged.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# preprocess numeric variables\n",
    "\n",
    "# flag errors in vax_date and onset_date\n",
    "df_merged.loc[\n",
    "    (df_merged['ONSET_DATE'] < pd.to_datetime('2020-01-01')) |\n",
    "    (df_merged['VAX_DATE'] < pd.to_datetime('2020-01-01')) | \n",
    "    ((df_merged['ONSET_DATE'] - df_merged['VAX_DATE']).astype('timedelta64[D]') != df_merged['NUMDAYS']),\n",
    "    ['VAX_DATE', 'ONSET_DATE']\n",
    "] = pd.NaT\n",
    "\n",
    "# flag errors in numdays\n",
    "df_merged.loc[\n",
    "    df_merged['ONSET_DATE'].isna() |\n",
    "    df_merged['VAX_DATE'].isna(),\n",
    "    'NUMDAYS'\n",
    "] = np.nan\n",
    "\n",
    "# drop ALL rows with na values in either numdays or age\n",
    "df_merged.dropna(subset=['NUMDAYS', 'AGE_YRS', 'SYMP_STR'], inplace=True)\n",
    "\n",
    "\n",
    "# preprocess categorical variables\n",
    "\n",
    "# VAX_MANU\n",
    "df_merged['MANU_PFIZER'] = np.where(df_merged['VAX_MANU'] == 'PFIZER\\BIONTECH', 1, 0)\n",
    "df_merged['MANU_MODERNA'] = np.where(df_merged['VAX_MANU'] == 'MODERNA', 1, 0)\n",
    "df_merged['MANU_JANSSEN'] = np.where(df_merged['VAX_MANU'] == 'JANSSEN', 1, 0)\n",
    "df_merged['MANU_NOVAVAX'] = np.where(df_merged['VAX_MANU'] == 'NOVAVAX', 1, 0)\n",
    "\n",
    "# GENDER\n",
    "df_merged['GENDER_F'] = np.where(df_merged['SEX']=='F', 1, 0)\n",
    "df_merged['GENDER_M'] = np.where(df_merged['SEX']=='M', 1, 0)\n",
    "\n",
    "# BIRTH_DEFECT\n",
    "df_merged['DEFECT'] = np.where(df_merged['BIRTH_DEFECT'] == 'Y', 1, 0)\n",
    "\n",
    "# HOSPITAL (response variable)\n",
    "df_merged['HOSP'] = np.where(df_merged['HOSPITAL'] == 'Y', 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f174ee8c",
   "metadata": {},
   "source": [
    "# Preprocess 'HISTORY' strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ec4e0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace na values in medical history\n",
    "df_merged['HISTORY'] = df_merged['HISTORY'].str.lower().str.strip()\n",
    "df_merged.loc[df_merged['HISTORY'].isna(), 'HISTORY'] = 'no known medical history'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sBERT model\n",
    "model = SentenceTransformer('pritamdeka/BioBERT-mnli-snli-scinli-scitail-mednli-stsb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify cases of no known medical history cases\n",
    "nkmh_phrases_count = df_merged.loc[df_merged['HISTORY'].astype(str).apply(len) <= 1000, 'HISTORY'].value_counts()\n",
    "nkmh_phrases = list(nkmh_phrases_count[nkmh_phrases_count>=5].index)\n",
    "\n",
    "nkmh_phrases_embeddings = model.encode(nkmh_phrases, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b604ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain embeddings for reference phrases\n",
    "nkmh_references_embeddings = model.encode([\n",
    "    'no known medical history',\n",
    "    'none reported',\n",
    "    'unknown',\n",
    "    'no adverse',\n",
    "    'unaware',\n",
    "    'none'\n",
    "], show_progress_bar=True)\n",
    "\n",
    "# normalise embeddings of reference phrases\n",
    "nkmh_references_embeddings = nkmh_references_embeddings / np.linalg.norm(nkmh_references_embeddings, axis=1)[:, None]\n",
    "\n",
    "# cosine similarities of all embeddings to reference phrases\n",
    "nkmh_cos_sim = np.matmul(nkmh_references_embeddings, (nkmh_phrases_embeddings / np.linalg.norm(nkmh_phrases_embeddings, axis=1)[:, None]).T)\n",
    "\n",
    "# rank by embeddings by cosine similarity to reference phrases to identify all synomymous phrases\n",
    "(\n",
    "    pd.DataFrame({\n",
    "        'Phrase': nkmh_phrases,\n",
    "        'Cosine Similarity with NKMH': np.max(nkmh_cos_sim, axis=0)\n",
    "    })\n",
    "    .sort_values('Cosine Similarity with NKMH', ascending=False)\n",
    "    .to_excel(os.path.join(DATA_DIR, 'nkmh_phrases.xlsx'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ff537e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract synonymous phrases to nkmh using annotated excel file\n",
    "# file is generated by the above code and annotated manually\n",
    "nkmh_df = pd.read_excel(os.path.join(DATA_DIR, 'nkmh_phrases_annotated.xlsx'), keep_default_na=False)\n",
    "HISTORY_NKMH_PHRASES = nkmh_df.loc[nkmh_df['NKMH']=='Y', 'Phrase'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20501b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then rename nkmh phrases\n",
    "df_merged.loc[df_merged['HISTORY'].isin(HISTORY_NKMH_PHRASES), 'HISTORY'] = 'no known medical history'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e6451",
   "metadata": {},
   "source": [
    "# FINALLY: Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6f698e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_X = 'NUMDAYS, AGE_YRS, GENDER_F, GENDER_M, MANU_PFIZER, MANU_MODERNA, MANU_JANSSEN, MANU_NOVAVAX, DEFECT, HISTORY, SYMP_STR'.split(', ')\n",
    "COLS_Y = 'HOSP'.split(', ')\n",
    "\n",
    "X, y = df_merged[COLS_X], df_merged[COLS_Y]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.85, random_state=55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a447273",
   "metadata": {},
   "source": [
    "# Interpret Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b6183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df_merged.sample(n=2000, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a1da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(df_sample['HISTORY'].to_list(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'n_neighbors': hp.choice('n_neighbors', [5, 15, 50]),\n",
    "    'n_components': hp.choice('n_components', [5, 10]),\n",
    "    'min_cluster_size': hp.choice('min_cluster_size', [5, 15, 50]),\n",
    "    'random_state': 5\n",
    "}\n",
    "\n",
    "best_params, best_clusters, trials = bayesian_search(embeddings, space, 5, 15, max_evals=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53542723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate clusters on sample df\n",
    "cluster = generate_clusters(embeddings, n_neighbors=5, n_components=5, min_cluster_size=15, random_state=5)\n",
    "top_n_words_dict = extract_top_n_words_by_topic(df_sample, 'HISTORY', cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2a7d55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualise clusters\n",
    "topic_df = generate_topic_df(df_sample, 'HISTORY', cluster)\n",
    "\n",
    "for topic in top_n_words_dict:\n",
    "    print(f'Topic {topic}')\n",
    "    print(top_n_words_dict[topic])\n",
    "    wc = WordCloud().generate(topic_df.loc[topic_df['topic']==topic, 'document'].iloc[0])\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65267e6c",
   "metadata": {},
   "source": [
    "umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_\n",
    "\n",
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
